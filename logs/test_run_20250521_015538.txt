2025-05-21 01:55:39,077 - __main__ - INFO - Found 31 models in configuration
2025-05-21 01:55:39,077 - __main__ - INFO - Testing 31 models: claude_3_opus, claude_3_5_sonnet, claude_3_5_haiku, claude_3_7_sonnet, gemini_2_5_pro_preview_05_06, gemini_2_5_pro_preview_03_25, gemini_2_5_pro_exp_03_25, gemini_2_5_flash_preview_04_17, gemini_2_5_flash_preview_04_17_thinking, gemini_2_0_pro_exp, gemini_2_0_flash, gemini_2_0_flash_thinking_exp, gemini_1_5_pro, gemini_1_5_flash, mistral_small, mistral_medium, mistral_large, gpt_4_5_preview, gpt_4o, gpt_4_1, gpt_4_turbo, o1_preview, o1_pro, command_a_03_2025, command_r_plus_08_2024, command_r_plus_04_2024, command_r_plus, command_r_08_2024_v004, command_r_08_2024_v003, command_r_03_2024, command_r
2025-05-21 01:55:39,105 - __main__ - INFO - Using gpt_4o for evaluation
2025-05-21 01:55:39,105 - src.test_runner.executor - INFO - Running image_prompts test with medium context on claude_3_opus
2025-05-21 01:55:39,105 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:39,112 - src.test_runner.executor - INFO - Running image_prompts test with medium context on claude_3_5_sonnet
2025-05-21 01:55:39,112 - src.test_runner.executor - INFO - Running image_prompts test with medium context on claude_3_5_haiku
2025-05-21 01:55:39,113 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:39,113 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:39,113 - src.test_runner.executor - INFO - Running image_prompts test with medium context on claude_3_7_sonnet
2025-05-21 01:55:39,113 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_5_pro_preview_05_06
2025-05-21 01:55:39,114 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:39,114 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:39,164 - src.test_runner.executor - INFO - Generating response from gemini_2_5_pro_preview_05_06
2025-05-21 01:55:39,932 - src.test_runner.executor - INFO - Generating response from claude_3_7_sonnet
2025-05-21 01:55:39,942 - src.test_runner.executor - INFO - Generating response from claude_3_5_sonnet
2025-05-21 01:55:39,959 - src.test_runner.executor - INFO - Generating response from claude_3_opus
2025-05-21 01:55:39,968 - src.test_runner.executor - INFO - Generating response from claude_3_5_haiku
2025-05-21 01:55:40,332 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:40,337 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}
2025-05-21 01:55:40,337 - src.test_runner.retry - INFO - Retrying in 2.06 seconds...
2025-05-21 01:55:40,397 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:40,397 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}
2025-05-21 01:55:40,397 - src.test_runner.retry - INFO - Retrying in 2.06 seconds...
2025-05-21 01:55:42,822 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:42,822 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}
2025-05-21 01:55:42,822 - src.test_runner.retry - INFO - Retrying in 4.12 seconds...
2025-05-21 01:55:43,067 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:43,067 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}
2025-05-21 01:55:43,067 - src.test_runner.retry - INFO - Retrying in 4.12 seconds...
2025-05-21 01:55:44,232 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 200 OK"
2025-05-21 01:55:47,343 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:47,345 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}
2025-05-21 01:55:47,345 - src.test_runner.retry - INFO - Retrying in 8.24 seconds...
2025-05-21 01:55:47,522 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:47,522 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}
2025-05-21 01:55:47,522 - src.test_runner.retry - INFO - Retrying in 8.24 seconds...
2025-05-21 01:55:54,604 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:55:54,632 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 200 OK"
2025-05-21 01:55:56,102 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:56,106 - src.test_runner.retry - ERROR - All retry attempts failed: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}
2025-05-21 01:55:56,106 - src.test_runner.executor - ERROR - Error running test for claude_3_7_sonnet: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\anthropic_client.py", line 38, in generate
    response = self.client.messages.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\_utils\_utils.py", line 283, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\resources\messages\messages.py", line 954, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\_base_client.py", line 1290, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\_base_client.py", line 1085, in request
    raise self._make_status_error_from_response(err.response) from None
anthropic.NotFoundError: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\anthropic_client.py", line 79, in generate
    raise Exception(f"Anthropic API error: {str(e)}")
Exception: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7-sonnet-20240620'}}
2025-05-21 01:55:56,110 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_5_pro_preview_03_25
2025-05-21 01:55:56,110 - src.test_runner.executor - INFO - Completed test for claude_3_7_sonnet, image_prompts, medium
2025-05-21 01:55:56,113 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:56,117 - src.test_runner.executor - INFO - Generating response from gemini_2_5_pro_preview_03_25
2025-05-21 01:55:56,558 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 404 Not Found"
2025-05-21 01:55:56,558 - src.test_runner.retry - ERROR - All retry attempts failed: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}
2025-05-21 01:55:56,558 - src.test_runner.executor - ERROR - Error running test for claude_3_5_haiku: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\anthropic_client.py", line 38, in generate
    response = self.client.messages.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\_utils\_utils.py", line 283, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\resources\messages\messages.py", line 954, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\_base_client.py", line 1290, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\anthropic\_base_client.py", line 1085, in request
    raise self._make_status_error_from_response(err.response) from None
anthropic.NotFoundError: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\anthropic_client.py", line 79, in generate
    raise Exception(f"Anthropic API error: {str(e)}")
Exception: Anthropic API error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-haiku-20240307'}}
2025-05-21 01:55:56,558 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_5_pro_exp_03_25
2025-05-21 01:55:56,558 - src.test_runner.executor - INFO - Completed test for claude_3_5_haiku, image_prompts, medium
2025-05-21 01:55:56,558 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:55:56,567 - src.test_runner.executor - INFO - Generating response from gemini_2_5_pro_exp_03_25
2025-05-21 01:55:58,295 - src.test_runner.retry - WARNING - Rate limit hit on attempt 1/4, retrying...
2025-05-21 01:55:58,295 - src.test_runner.retry - INFO - Retrying in 2.10 seconds...
2025-05-21 01:56:03,439 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:03,448 - openai._base_client - INFO - Retrying request to /chat/completions in 0.487500 seconds
2025-05-21 01:56:03,607 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:03,607 - openai._base_client - INFO - Retrying request to /chat/completions in 0.415291 seconds
2025-05-21 01:56:03,688 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:03,700 - openai._base_client - INFO - Retrying request to /chat/completions in 0.390141 seconds
2025-05-21 01:56:03,973 - openai._base_client - INFO - Retrying request to /chat/completions in 0.942507 seconds
2025-05-21 01:56:04,052 - openai._base_client - INFO - Retrying request to /chat/completions in 0.978862 seconds
2025-05-21 01:56:04,127 - openai._base_client - INFO - Retrying request to /chat/completions in 0.896235 seconds
2025-05-21 01:56:04,943 - openai._base_client - INFO - Retrying request to /chat/completions in 0.434587 seconds
2025-05-21 01:56:05,082 - openai._base_client - INFO - Retrying request to /chat/completions in 0.450638 seconds
2025-05-21 01:56:05,108 - openai._base_client - INFO - Retrying request to /chat/completions in 0.438667 seconds
2025-05-21 01:56:05,384 - openai._base_client - INFO - Retrying request to /chat/completions in 0.827303 seconds
2025-05-21 01:56:05,561 - openai._base_client - INFO - Retrying request to /chat/completions in 0.786242 seconds
2025-05-21 01:56:05,582 - openai._base_client - INFO - Retrying request to /chat/completions in 0.932425 seconds
2025-05-21 01:56:06,218 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_5_flash_preview_04_17
2025-05-21 01:56:06,218 - src.test_runner.executor - INFO - Completed test for claude_3_5_sonnet, image_prompts, medium
2025-05-21 01:56:06,218 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:56:06,233 - src.test_runner.executor - INFO - Generating response from gemini_2_5_flash_preview_04_17
2025-05-21 01:56:06,396 - openai._base_client - INFO - Retrying request to /chat/completions in 0.398700 seconds
2025-05-21 01:56:06,566 - openai._base_client - INFO - Retrying request to /chat/completions in 0.473025 seconds
2025-05-21 01:56:06,816 - openai._base_client - INFO - Retrying request to /chat/completions in 0.926868 seconds
2025-05-21 01:56:07,055 - openai._base_client - INFO - Retrying request to /chat/completions in 0.908124 seconds
2025-05-21 01:56:07,771 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_5_flash_preview_04_17_thinking
2025-05-21 01:56:07,771 - src.test_runner.executor - INFO - Completed test for claude_3_opus, image_prompts, medium
2025-05-21 01:56:07,772 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:56:07,787 - src.test_runner.executor - INFO - Generating response from gemini_2_5_flash_preview_04_17_thinking
2025-05-21 01:56:07,991 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_0_pro_exp
2025-05-21 01:56:07,991 - src.test_runner.executor - INFO - Completed test for gemini_2_5_pro_preview_05_06, image_prompts, medium
2025-05-21 01:56:07,991 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:56:07,999 - src.test_runner.executor - INFO - Generating response from gemini_2_0_pro_exp
2025-05-21 01:56:12,893 - openai._base_client - INFO - Retrying request to /chat/completions in 0.483407 seconds
2025-05-21 01:56:13,443 - openai._base_client - INFO - Retrying request to /chat/completions in 0.810394 seconds
2025-05-21 01:56:22,768 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:27,999 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:32,585 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:32,640 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_0_flash
2025-05-21 01:56:32,642 - src.test_runner.executor - INFO - Completed test for gemini_2_5_pro_preview_03_25, image_prompts, medium
2025-05-21 01:56:32,642 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:56:32,650 - src.test_runner.executor - INFO - Generating response from gemini_2_0_flash
2025-05-21 01:56:38,178 - src.test_runner.retry - WARNING - Rate limit hit on attempt 1/4, retrying...
2025-05-21 01:56:38,178 - src.test_runner.retry - INFO - Retrying in 2.16 seconds...
2025-05-21 01:56:39,183 - src.test_runner.retry - WARNING - Rate limit hit on attempt 2/4, retrying...
2025-05-21 01:56:39,183 - src.test_runner.retry - INFO - Retrying in 4.32 seconds...
2025-05-21 01:56:40,668 - src.test_runner.retry - WARNING - Rate limit hit on attempt 2/4, retrying...
2025-05-21 01:56:40,668 - src.test_runner.retry - INFO - Retrying in 4.32 seconds...
2025-05-21 01:56:43,849 - src.test_runner.retry - WARNING - Rate limit hit on attempt 3/4, retrying...
2025-05-21 01:56:43,849 - src.test_runner.retry - INFO - Retrying in 8.24 seconds...
2025-05-21 01:56:44,484 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:45,324 - src.test_runner.retry - WARNING - Rate limit hit on attempt 3/4, retrying...
2025-05-21 01:56:45,324 - src.test_runner.retry - INFO - Retrying in 8.64 seconds...
2025-05-21 01:56:49,819 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:51,404 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:52,414 - src.test_runner.retry - ERROR - All retry attempts failed: Google API error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 9
}
]
2025-05-21 01:56:52,414 - src.test_runner.executor - ERROR - Error running test for gemini_2_5_pro_exp_03_25: Google API error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 9
}
]
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\google_client.py", line 42, in generate
    response = model.generate_content(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 9
}
]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\google_client.py", line 81, in generate
    raise Exception(f"Google API error: {str(e)}")
Exception: Google API error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 9
}
]
2025-05-21 01:56:52,423 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_2_0_flash_thinking_exp
2025-05-21 01:56:52,423 - src.test_runner.executor - INFO - Completed test for gemini_2_5_pro_exp_03_25, image_prompts, medium
2025-05-21 01:56:52,423 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:56:52,432 - src.test_runner.executor - INFO - Generating response from gemini_2_0_flash_thinking_exp
2025-05-21 01:56:54,306 - src.test_runner.retry - ERROR - All retry attempts failed: Google API error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 7
}
]
2025-05-21 01:56:54,306 - src.test_runner.executor - ERROR - Error running test for gemini_2_0_pro_exp: Google API error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 7
}
]
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\google_client.py", line 42, in generate
    response = model.generate_content(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\generativeai\generative_models.py", line 331, in generate_content
    response = self._client.generate_content(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 835, in generate_content
    response = rpc(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\google\api_core\grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 7
}
]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\google_client.py", line 81, in generate
    raise Exception(f"Google API error: {str(e)}")
Exception: Google API error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model_per_day"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel"
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_requests_per_model"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 7
}
]
2025-05-21 01:56:54,306 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_1_5_pro
2025-05-21 01:56:54,306 - src.test_runner.executor - INFO - Completed test for gemini_2_0_pro_exp, image_prompts, medium
2025-05-21 01:56:54,306 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:56:54,319 - src.test_runner.executor - INFO - Generating response from gemini_1_5_pro
2025-05-21 01:56:55,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:56:57,045 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:01,496 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:01,561 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gemini_1_5_flash
2025-05-21 01:57:01,561 - src.test_runner.executor - INFO - Completed test for gemini_2_0_flash, image_prompts, medium
2025-05-21 01:57:01,561 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:01,573 - src.test_runner.executor - INFO - Generating response from gemini_1_5_flash
2025-05-21 01:57:07,378 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:07,389 - src.test_runner.executor - INFO - Running image_prompts test with medium context on mistral_small
2025-05-21 01:57:07,389 - src.test_runner.executor - INFO - Completed test for gemini_2_5_flash_preview_04_17, image_prompts, medium
2025-05-21 01:57:07,389 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:07,399 - src.utils.config - ERROR - Error creating client for model mistral_small: This client is deprecated. To migrate to the new client, please refer to this guide: https://github.com/mistralai/client-python/blob/main/MIGRATION.md. If you need to use this client anyway, pin your version to 0.4.2.
2025-05-21 01:57:07,399 - src.test_runner.executor - INFO - Running image_prompts test with medium context on mistral_medium
2025-05-21 01:57:07,399 - src.test_runner.executor - INFO - Completed test for mistral_small, image_prompts, medium
2025-05-21 01:57:07,399 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:07,411 - src.utils.config - ERROR - Error creating client for model mistral_medium: This client is deprecated. To migrate to the new client, please refer to this guide: https://github.com/mistralai/client-python/blob/main/MIGRATION.md. If you need to use this client anyway, pin your version to 0.4.2.
2025-05-21 01:57:07,411 - src.test_runner.executor - INFO - Running image_prompts test with medium context on mistral_large
2025-05-21 01:57:07,411 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:07,411 - src.test_runner.executor - INFO - Completed test for mistral_medium, image_prompts, medium
2025-05-21 01:57:07,420 - src.utils.config - ERROR - Error creating client for model mistral_large: This client is deprecated. To migrate to the new client, please refer to this guide: https://github.com/mistralai/client-python/blob/main/MIGRATION.md. If you need to use this client anyway, pin your version to 0.4.2.
2025-05-21 01:57:07,420 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gpt_4_5_preview
2025-05-21 01:57:07,420 - src.test_runner.executor - INFO - Completed test for mistral_large, image_prompts, medium
2025-05-21 01:57:07,420 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:07,842 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:07,889 - src.test_runner.executor - INFO - Generating response from gpt_4_5_preview
2025-05-21 01:57:08,373 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:08,375 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:08,375 - src.test_runner.retry - INFO - Retrying in 2.02 seconds...
2025-05-21 01:57:10,662 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:10,801 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:10,803 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:10,803 - src.test_runner.retry - INFO - Retrying in 4.04 seconds...
2025-05-21 01:57:12,968 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:13,406 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:14,070 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:15,255 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:15,258 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:15,258 - src.test_runner.retry - INFO - Retrying in 8.08 seconds...
2025-05-21 01:57:19,426 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:19,475 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gpt_4o
2025-05-21 01:57:19,475 - src.test_runner.executor - INFO - Completed test for gemini_2_5_flash_preview_04_17_thinking, image_prompts, medium
2025-05-21 01:57:19,475 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:19,909 - src.test_runner.executor - INFO - Generating response from gpt_4o
2025-05-21 01:57:21,408 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:21,409 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:21,409 - src.test_runner.retry - INFO - Retrying in 2.02 seconds...
2025-05-21 01:57:21,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:21,539 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gpt_4_1
2025-05-21 01:57:21,539 - src.test_runner.executor - INFO - Completed test for gemini_2_0_flash_thinking_exp, image_prompts, medium
2025-05-21 01:57:21,539 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:21,983 - src.test_runner.executor - INFO - Generating response from gpt_4_1
2025-05-21 01:57:22,662 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:22,663 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:22,663 - src.test_runner.retry - INFO - Retrying in 2.16 seconds...
2025-05-21 01:57:23,781 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:23,781 - src.test_runner.retry - ERROR - All retry attempts failed: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:23,781 - src.test_runner.executor - ERROR - Error running test for gpt_4_5_preview: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 44, in generate
    response = self.client.chat.completions.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 77, in generate
    raise Exception(f"OpenAI API error: {str(e)}")
Exception: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:23,788 - src.test_runner.executor - INFO - Running image_prompts test with medium context on gpt_4_turbo
2025-05-21 01:57:23,788 - src.test_runner.executor - INFO - Completed test for gpt_4_5_preview, image_prompts, medium
2025-05-21 01:57:23,788 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:23,828 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:23,828 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:23,828 - src.test_runner.retry - INFO - Retrying in 4.04 seconds...
2025-05-21 01:57:24,258 - src.test_runner.executor - INFO - Generating response from gpt_4_turbo
2025-05-21 01:57:25,190 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:25,190 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:25,190 - src.test_runner.retry - INFO - Retrying in 4.32 seconds...
2025-05-21 01:57:25,242 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:25,242 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:25,242 - src.test_runner.retry - INFO - Retrying in 2.10 seconds...
2025-05-21 01:57:26,934 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:27,748 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:27,753 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:27,753 - src.test_runner.retry - INFO - Retrying in 4.20 seconds...
2025-05-21 01:57:28,470 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:28,472 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:28,472 - src.test_runner.retry - INFO - Retrying in 8.08 seconds...
2025-05-21 01:57:29,888 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:29,888 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:29,888 - src.test_runner.retry - INFO - Retrying in 8.64 seconds...
2025-05-21 01:57:31,865 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:32,349 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:32,349 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:32,351 - src.test_runner.retry - INFO - Retrying in 8.40 seconds...
2025-05-21 01:57:33,702 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:33,751 - src.test_runner.executor - INFO - Running image_prompts test with medium context on o1_preview
2025-05-21 01:57:33,752 - src.test_runner.executor - INFO - Completed test for gemini_1_5_flash, image_prompts, medium
2025-05-21 01:57:33,752 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:34,345 - src.test_runner.executor - INFO - Generating response from o1_preview
2025-05-21 01:57:34,789 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:34,789 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-21 01:57:34,789 - src.test_runner.retry - INFO - Retrying in 2.12 seconds...
2025-05-21 01:57:37,060 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:37,065 - src.test_runner.retry - ERROR - All retry attempts failed: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:37,065 - src.test_runner.executor - ERROR - Error running test for gpt_4o: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 44, in generate
    response = self.client.chat.completions.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 77, in generate
    raise Exception(f"OpenAI API error: {str(e)}")
Exception: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 16384 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:37,065 - src.test_runner.executor - INFO - Running image_prompts test with medium context on o1_pro
2025-05-21 01:57:37,065 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:37,065 - src.test_runner.executor - INFO - Completed test for gpt_4o, image_prompts, medium
2025-05-21 01:57:37,269 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:37,269 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-21 01:57:37,269 - src.test_runner.retry - INFO - Retrying in 4.24 seconds...
2025-05-21 01:57:37,512 - src.test_runner.executor - INFO - Generating response from o1_pro
2025-05-21 01:57:37,867 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-05-21 01:57:37,868 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: OpenAI API error: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
2025-05-21 01:57:37,868 - src.test_runner.retry - INFO - Retrying in 2.08 seconds...
2025-05-21 01:57:39,001 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:39,001 - src.test_runner.retry - ERROR - All retry attempts failed: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:39,001 - src.test_runner.executor - ERROR - Error running test for gpt_4_1: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 44, in generate
    response = self.client.chat.completions.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 77, in generate
    raise Exception(f"OpenAI API error: {str(e)}")
Exception: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 32768 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:39,003 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_a_03_2025
2025-05-21 01:57:39,003 - src.test_runner.executor - INFO - Completed test for gpt_4_1, image_prompts, medium
2025-05-21 01:57:39,003 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:39,486 - src.test_runner.executor - INFO - Generating response from command_a_03_2025
2025-05-21 01:57:39,559 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-21 01:57:39,612 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r_plus_08_2024
2025-05-21 01:57:39,614 - src.test_runner.executor - INFO - Completed test for gemini_1_5_pro, image_prompts, medium
2025-05-21 01:57:39,614 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:40,073 - src.test_runner.executor - INFO - Generating response from command_r_plus_08_2024
2025-05-21 01:57:40,102 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:40,102 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}
2025-05-21 01:57:40,102 - src.test_runner.retry - INFO - Retrying in 2.12 seconds...
2025-05-21 01:57:40,292 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-05-21 01:57:40,292 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: OpenAI API error: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
2025-05-21 01:57:40,292 - src.test_runner.retry - INFO - Retrying in 4.16 seconds...
2025-05-21 01:57:40,470 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:40,470 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:40,470 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:41,200 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:41,201 - src.test_runner.retry - ERROR - All retry attempts failed: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:41,202 - src.test_runner.executor - ERROR - Error running test for gpt_4_turbo: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 44, in generate
    response = self.client.chat.completions.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 77, in generate
    raise Exception(f"OpenAI API error: {str(e)}")
Exception: OpenAI API error: Error code: 400 - {'error': {'message': 'max_tokens is too large: 128000. This model supports at most 4096 completion tokens, whereas you provided 128000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'invalid_value'}}
2025-05-21 01:57:41,202 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r_plus_04_2024
2025-05-21 01:57:41,202 - src.test_runner.executor - INFO - Completed test for gpt_4_turbo, image_prompts, medium
2025-05-21 01:57:41,202 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:41,656 - src.test_runner.executor - INFO - Generating response from command_r_plus_04_2024
2025-05-21 01:57:41,869 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:41,869 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-21 01:57:41,869 - src.test_runner.retry - INFO - Retrying in 8.48 seconds...
2025-05-21 01:57:42,055 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:42,055 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:42,055 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:42,555 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:42,555 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}
2025-05-21 01:57:42,555 - src.test_runner.retry - INFO - Retrying in 4.24 seconds...
2025-05-21 01:57:42,926 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:42,926 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:42,926 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:57:44,526 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:44,526 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:44,526 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:57:44,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-05-21 01:57:44,945 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: OpenAI API error: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
2025-05-21 01:57:44,945 - src.test_runner.retry - INFO - Retrying in 8.32 seconds...
2025-05-21 01:57:47,116 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:47,120 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}
2025-05-21 01:57:47,120 - src.test_runner.retry - INFO - Retrying in 8.48 seconds...
2025-05-21 01:57:47,549 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:47,549 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:47,549 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:57:49,132 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:49,132 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:49,132 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:57:50,804 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:50,806 - src.test_runner.retry - ERROR - All retry attempts failed: OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-21 01:57:50,806 - src.test_runner.executor - ERROR - Error running test for o1_preview: OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 44, in generate
    response = self.client.chat.completions.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 77, in generate
    raise Exception(f"OpenAI API error: {str(e)}")
Exception: OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-21 01:57:50,806 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r_plus
2025-05-21 01:57:50,806 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:50,806 - src.test_runner.executor - INFO - Completed test for o1_preview, image_prompts, medium
2025-05-21 01:57:51,432 - src.test_runner.executor - INFO - Generating response from command_r_plus
2025-05-21 01:57:51,851 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:51,851 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:51,851 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:53,650 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-05-21 01:57:53,652 - src.test_runner.retry - ERROR - All retry attempts failed: OpenAI API error: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
2025-05-21 01:57:53,652 - src.test_runner.executor - ERROR - Error running test for o1_pro: OpenAI API error: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 44, in generate
    response = self.client.chat.completions.create(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\openai\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\openai_client.py", line 77, in generate
    raise Exception(f"OpenAI API error: {str(e)}")
Exception: OpenAI API error: Error code: 404 - {'error': {'message': 'This model is only supported in v1/responses and not in v1/chat/completions.', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}
2025-05-21 01:57:53,652 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r_08_2024_v004
2025-05-21 01:57:53,654 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:53,654 - src.test_runner.executor - INFO - Completed test for o1_pro, image_prompts, medium
2025-05-21 01:57:54,068 - src.test_runner.executor - INFO - Generating response from command_r_08_2024_v004
2025-05-21 01:57:54,324 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:54,326 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:54,326 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:57:54,459 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:54,459 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:54,463 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:56,010 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:56,016 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}
2025-05-21 01:57:56,016 - src.test_runner.executor - ERROR - Error running test for command_a_03_2025: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 8192, the maximum output for this model - received 128000.'}
2025-05-21 01:57:56,021 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r_08_2024_v003
2025-05-21 01:57:56,021 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:56,021 - src.test_runner.executor - INFO - Completed test for command_a_03_2025, image_prompts, medium
2025-05-21 01:57:56,472 - src.test_runner.executor - INFO - Generating response from command_r_08_2024_v003
2025-05-21 01:57:56,517 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:56,533 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:56,533 - src.test_runner.executor - ERROR - Error running test for command_r_plus_08_2024: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:56,535 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r_03_2024
2025-05-21 01:57:56,535 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:56,535 - src.test_runner.executor - INFO - Completed test for command_r_plus_08_2024, image_prompts, medium
2025-05-21 01:57:56,860 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:56,866 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:56,866 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:56,925 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:56,925 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:56,926 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:57:57,032 - src.test_runner.executor - INFO - Generating response from command_r_03_2024
2025-05-21 01:57:57,433 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:57,436 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:57,436 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:58,108 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:58,108 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:58,108 - src.test_runner.executor - ERROR - Error running test for command_r_plus_04_2024: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:58,108 - src.test_runner.executor - INFO - Running image_prompts test with medium context on command_r
2025-05-21 01:57:58,108 - src.test_runner.executor - INFO - Completed test for command_r_plus_04_2024, image_prompts, medium
2025-05-21 01:57:58,108 - src.test_runner.executor - INFO - Loading context from: data\contexts\medium\contexts.json
2025-05-21 01:57:58,556 - src.test_runner.executor - INFO - Generating response from command_r
2025-05-21 01:57:58,998 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:59,000 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:59,000 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:59,001 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:57:59,001 - src.test_runner.retry - WARNING - Error on attempt 1/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:59,001 - src.test_runner.retry - INFO - Retrying in 2.14 seconds...
2025-05-21 01:57:59,336 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:59,337 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:59,337 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:57:59,903 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:57:59,903 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:57:59,905 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:58:01,471 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:01,471 - src.test_runner.retry - WARNING - Error on attempt 2/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:01,471 - src.test_runner.retry - INFO - Retrying in 4.28 seconds...
2025-05-21 01:58:01,526 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:01,528 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:01,528 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:58:03,942 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:03,942 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:03,942 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:58:04,510 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:04,510 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:04,510 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:58:06,076 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:06,076 - src.test_runner.retry - WARNING - Error on attempt 3/4, retrying: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:06,076 - src.test_runner.retry - INFO - Retrying in 8.56 seconds...
2025-05-21 01:58:07,971 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:07,971 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:07,971 - src.test_runner.executor - ERROR - Error running test for command_r_plus: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:07,975 - src.test_runner.executor - INFO - Completed test for command_r_plus, image_prompts, medium
2025-05-21 01:58:10,518 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:10,518 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:10,518 - src.test_runner.executor - ERROR - Error running test for command_r_08_2024_v004: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:10,519 - src.test_runner.executor - INFO - Completed test for command_r_08_2024_v004, image_prompts, medium
2025-05-21 01:58:12,913 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:12,913 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:12,913 - src.test_runner.executor - ERROR - Error running test for command_r_08_2024_v003: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:12,913 - src.test_runner.executor - INFO - Completed test for command_r_08_2024_v003, image_prompts, medium
2025-05-21 01:58:13,463 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:13,465 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:13,465 - src.test_runner.executor - ERROR - Error running test for command_r_03_2024: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:13,466 - src.test_runner.executor - INFO - Completed test for command_r_03_2024, image_prompts, medium
2025-05-21 01:58:15,043 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/chat "HTTP/1.1 400 Bad Request"
2025-05-21 01:58:15,043 - src.test_runner.retry - ERROR - All retry attempts failed: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:15,043 - src.test_runner.executor - ERROR - Error running test for command_r: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 39, in generate
    response = self.client.chat(
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 103, in _wrapped
    return func(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\client.py", line 35, in _wrapped
    return method(*args, **kwargs)
  File "C:\Users\Abhimanyu\anaconda3\envs\testing\lib\site-packages\cohere\base_client.py", line 998, in chat
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 179, in run_test
    response = self.retry_handler.with_retry(
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 106, in with_retry
    raise last_exception
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\retry.py", line 50, in with_retry
    return func(*args, **kwargs)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\test_runner\executor.py", line 180, in <lambda>
    lambda: client.generate(full_prompt, model_config),
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\clients\others.py", line 83, in generate
    raise Exception(f"Cohere API error: {str(e)}")
Exception: Cohere API error: status_code: 400, body: {'message': 'too many tokens: max tokens must be less than or equal to 4096, the maximum output for this model - received 128000.'}
2025-05-21 01:58:15,045 - src.test_runner.executor - INFO - Completed test for command_r, image_prompts, medium
2025-05-21 01:58:15,046 - src.reporting.html_generator - ERROR - Error generating HTML report: '>' not supported between instances of 'NoneType' and 'float'
Traceback (most recent call last):
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\reporting\html_generator.py", line 35, in generate_report
    html_content = self._generate_html(results)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\reporting\html_generator.py", line 181, in _generate_html
    html += self._generate_summary_table(results)
  File "D:\Testing\Model-Testing-Manual-Framework-main\src\reporting\html_generator.py", line 238, in _generate_summary_table
    score_class = "high" if score > 0.8 else "medium" if score > 0.5 else "low"
TypeError: '>' not supported between instances of 'NoneType' and 'float'
2025-05-21 01:58:15,047 - __main__ - INFO - Results saved to ./results\image_prompts\test_results_medium_20250521_015815.html
2025-05-21 01:58:15,100 - __main__ - INFO - Comparison report saved to ./results\comparisons\comparison_image_prompts_medium_20250521_015815.yaml
2025-05-21 01:58:16,138 - src.reporting.visualizations_reporter - INFO - Generated 1 visualizations in ./results\visualizations
2025-05-21 01:58:16,138 - __main__ - INFO - Visualizations generated in ./results\visualizations
2025-05-21 01:58:16,138 - __main__ - INFO - Testing completed successfully
